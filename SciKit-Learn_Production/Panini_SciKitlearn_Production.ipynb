{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy SciKit-Learn Model to Production using Panini in 3 minutes\n",
    "###### https://panini.ai/\n",
    "###### Panini is a platform that serves ML/DL models at low latency and makes the ML model deployment to production from a few days to a few minutes. \n",
    "\n",
    "Once deployed in Panini’s server, it will provide you with an API key to infer the model. Panini query engine is developed in C++, which provides very low latency during model inference and Kubernetes cluster is being used to store the model so, it is scalable to multiple nodes. Panini also takes care of caching and batching inputs during model inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's deploy the classic regression to Classify different flowers using the Iris dataset\n",
    "\n",
    "This data sets consists of 3 different types of irises’ (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avin/anaconda3/envs/clipper/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/avin/anaconda3/envs/clipper/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the Model\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pickle\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :] \n",
    "Y = iris.target\n",
    "model = linear_model.LogisticRegression(C=1e5)\n",
    "model.fit(X,Y)\n",
    "\n",
    "#Once the model is trained, save it using pickle.\n",
    "filename = 'model.pkl'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(model,outfile)\n",
    "outfile.close()\n",
    "\n",
    "#Load the model back, just to make sure everything is good!\n",
    "infile = open(filename,'rb')\n",
    "model = pickle.load(infile)\n",
    "infile.close()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You need to upload at least 3 files to panini:\n",
    " - predict.py\n",
    " - requirements.txt\n",
    " - model.pth¶\n",
    " \n",
    "### predict.py should have two functions:\n",
    "\n",
    " - def load(path): path -> saved state_dict has to be stored as .pth\n",
    "     returns model.\n",
    "     \n",
    "     load function is used to load the model. For our demo, we saved the model as \"model.pkl\"\n",
    "     Once the model is loaded, it will return back the model.\n",
    "     \n",
    "     \n",
    " - def predict(input_from_user): -> inputs_from_user is a list[]\n",
    "     returns prediction as a list\n",
    "     \n",
    "     This function gets executed, when a user sends POST request to our API. User's data is stored in the variable \"input_from_client\". It will return the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requirements.txt\n",
    "Create a file called \"requirements.txt\" and write these two entries!\n",
    "\n",
    "scikit-learn\n",
    "\n",
    "numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict.py\n",
    "\n",
    "def load(path):\n",
    "    import pickle\n",
    "    infile = open(path,'rb')\n",
    "    model = pickle.load(infile)\n",
    "    infile.close()\n",
    "    return model\n",
    "\n",
    "\n",
    "#Main predict() function!\n",
    "def predict(input_from_client):\n",
    "    #import sklearn\n",
    "    from sklearn import linear_model, datasets\n",
    "\n",
    "    from numpy import array\n",
    "    \n",
    "    #load our model. We don't need load() function but it's nice to have. \n",
    "    model = load(\"model.pkl\")\n",
    "    \n",
    "    #Once the model is loaded, feed the client's data into our model\n",
    "    prediction = model.predict(input_from_client)\n",
    "    value = []\n",
    "    for label in prediction:\n",
    "        if label == 0:\n",
    "            value.append('Setosa')\n",
    "        elif label == 1:\n",
    "            value.append('Virginica')\n",
    "        else:\n",
    "            value.append('Versicolour')\n",
    "    #Return prediction back to the client.\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Setosa', 'Versicolour']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict([X[0], X[101]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to panini.ai\n",
    " - Sign into https://panini.ai/\n",
    " \n",
    "\n",
    " - Give your model a name.\n",
    " \n",
    "\n",
    " - Choose input as floats\n",
    " \n",
    "\n",
    " - Upload: predict.py, requirements.txt and model.pkl\n",
    " \n",
    "\n",
    " - Do not upload anything for extrafiles (optional)\n",
    "\n",
    "\n",
    " - Press Deploy!\n",
    " \n",
    "\n",
    " - Refresh the page, and wait for API URL to appear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer using Panini.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_id': 51, 'output': 'Setosa', 'default': False}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Send POST request to API URL\n",
    "import requests, json\n",
    "\n",
    "#our API_URL\n",
    "API_LINK = \"https://api.panini.ai/jm0gox1kqty3xjni83fxahcf6ks2-scikit7/predict\"\n",
    "\n",
    "#Send Sepal Length, Sepal Width, Petal Length and Petal Width as input to classify.\n",
    "data_to_send = [5.1, 3.5, 1.4, 0.2]\n",
    "response = requests.post(\n",
    "     API_LINK,\n",
    "     headers={\"Content-type\": \"application/json\"},\n",
    "     data=json.dumps({\n",
    "         'input': data_to_send,\n",
    "     }))\n",
    "result = response.json()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"input\": [5.1, 3.5, 1.4, 0.2]}'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
